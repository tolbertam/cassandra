= Overview of Auto Repair
:navtitle: Auto Repair overview
:description: Auto Repair concepts - How it works, how to configure it, and more.
:keywords: CEP-37

Auto Repair is a fully automated scheduler that provides repair orchestration within Apache Cassandra. This
significantly reduces operational overhead by eliminating the need for operators to deploy external tools to submit and
manage repairs.

At a high level, a dedicated thread pool is assigned to the repair scheduler. The repair scheduler in Cassandra
maintains a new replicated table, system_distributed.auto_repair_history, which stores the repair history for all nodes,
including details such as the last repair time. The scheduler selects the node(s) to begin repairs and orchestrates the
process to ensure that every table and its token ranges are repaired. The algorithm can run repairs simultaneously on
multiple nodes and splits token ranges into subranges, with necessary retries to handle transient failures. Automatic
repair starts as soon as a Cassandra cluster is launched, similar to compaction, and does not require human
intervention.

The scheduler currently supports Full, Incremental, and Preview repair types with the following features. New repair
types, such as Paxos repair or other future repair mechanisms, can be integrated with minimal development effort!


=== Features
- Capability to run repairs on multiple nodes simultaneously.
- A default implementation and an interface to override the dataset being repaired per session.
- Extendable token split algorithms with two implementations readily available:
.  Splits token ranges by placing a cap on the size of data repaired in one session and a maximum cap at the schedule
level (default).
.  Splits tokens evenly based on the specified number of splits.
- A new CQL table property offering:
.  The ability to disable specific repair types at the table level, allowing the scheduler to skip one or more tables.
.  Configuring repair priorities for certain tables to prioritize them over others.
- Dynamic enablement or disablement of the scheduler for each repair type.
- Configurable settings tailored to each repair job.
- Rich configuration options for each repair type (e.g., Full, Incremental, or Preview repairs).
- Comprehensive observability features that allow operators to configure alarms as needed.

=== Yaml Configuration

==== Top level settings
The following settings are defined at the top level of the configuration file and apply universally across all
repair types.

[cols=",,",options="header",]
|===
| Name | Default | Description
| enabled | false | Enable/Disable the auto-repair scheduler. If set to false, the scheduler thread will not be started.
If set to true, the repair scheduler thread will be created. The thread will check for secondary configuration available
for each repair type (full, incremental, and preview_repaired), and based on that, it will schedule repairs.
| repair_check_interval | 5m | Time interval between successive checks to see if ongoing repairs are complete or if it
is time to schedule repairs.
| repair_max_retries | 3 | Maximum number of retries for a repair session.
| repair_retry_backoff | 30s | Backoff time before retrying a repair session.
| repair_task_min_duration | 5s | Minimum duration for the execution of a single repair task. This prevents the
scheduler from overwhelming the node by scheduling too many repair tasks in a short period of time.
| history_clear_delete_hosts_buffer_interval | 2h | The scheduler needs to adjust its order when nodes leave the ring.
Deleted hosts are tracked in metadata. for a specified duration to ensure they are indeed removed before adjustments
are made to the schedule.
|===


==== Repair level settings
The following settings can be tailored individually for each repair type.
[cols=",,",options="header",]
|===
| Name | Default | Description
| enabled | false | Enable/Disable full/incremental/preview_repaired auto-repair
| repair_by_keyspace | false | If true, attempts to group tables in the same keyspace into one repair; otherwise,
each table is repaired individually.
| number_of_repair_threads | 1 | Number of threads to use for each repair job scheduled by the scheduler. Similar to
the -j option in nodetool repair.
| parallel_repair_count | 3 | Number of nodes running repair in parallel. If parallel_repair_percentage is set, the
larger value is used.
| parallel_repair_percentage | 3 | Percentage of nodes in the cluster running repair in parallel. If
parallel_repair_count is set, the larger value is used. Recommendation is that the repair cycle on the cluster should
finish within gc_grace_seconds.
| materialized_view_repair_enabled | false | Repairs materialized views if true.
| initial_scheduler_delay | 5m | Delay before starting repairs after a node restarts to avoid repairs starting
immediately after a restart.
| repair_session_timeout | 3h | Timeout for resuming stuck repair sessions.
| force_repair_new_node | false | Force immediate repair on new nodes after they join the ring.
| sstable_upper_threshold | 10000 | Threshold to skip repairing tables with too many SSTables. Defaults to 10,000
SSTables to avoid penalizing good tables.
| table_max_repair_time | 6h | Maximum time allowed for repairing one table on a given node. If exceeded, the repair
proceeds to the next table.
| ignore_dcs | [] | Avoid running repairs in specific data centers. By default, repairs run in all data centers. Specify
data centers to exclude in this list. Note that repair sessions will still consider all replicas from excluded data
centers. Useful if you have keyspaces that are not replicated in certain data centers, and you want to not run repair
schedule in certain data centers.
| repair_primary_token_range_only | true | Repair only the primary ranges owned by a node. Equivalent to the -pr option
in nodetool repair. Defaults to true. General advice is to keep this true.
| token_range_splitter | org.apache.cassandra.repair.autorepair.RepairTokenRangeSplitter | Splitter implementation to
generate repair assignments. Defaults to RepairTokenRangeSplitter.
| token_range_splitter.partitions_per_assignment | 1048576 | Target number of partitions to include in a repair
assignment to reduce excessive overstreaming.
| token_range_splitter.max_tables_per_assignment | 64 | Maximum number of tables to include in a repair assignment.
This reduces the number of repairs, especially in keyspaces with many tables. The splitter avoids batching tables
together if they exceed other configuration parameters like bytes_per_assignment or partitions_per_assignment.
|===

==== Full & Preview_Repaired repair level settings
The following settings can be tailored individually for each repair type.
[cols=",,",options="header",]
|===
| min_repair_interval | 24h | Minimum duration between repairing the same node again. This is useful for tiny clusters,
such as clusters with 5 nodes that finish repairs quickly. The default is 24 hours. This means that if the scheduler
completes one round on all nodes in less than 24 hours, it will not start a new repair round on a given node until 24
hours have passed since the last repair.
| token_range_splitter.bytes_per_assignment | 200GiB | The target and maximum amount of bytes that should be included
in a repair assignment. This scopes the amount of work involved in a repair and includes the data covering the range
being repaired.
| token_range_splitter.max_bytes_per_schedule | 100000GiB | The maximum number of bytes to cover in an individual
schedule. This serves as a mechanism to throttle the work done in each repair cycle. You may reduce this value if the
impact of repairs is causing too much load on the cluster or increase it if writes outpace the amount of data being
repaired. Alternatively, adjust the min_repair_interval. This is set to a large value for full repair to attempt to
repair all data per repair schedule.
|===

==== Incremental repair level settings
The following settings can be customized for each repair type.
[cols=",,",options="header",]
|===
| min_repair_interval | 1h | Incremental repairs operate over unrepaired data and should finish quickly. Running them
more frequently keeps the unrepaired set smaller and thus causes repairs to operate over a smaller set of data,
so a more frequent schedule such as 1h is recommended. When turning on incremental repair for the first time with a
decent amount of data it may be advisable to increase this interval to 24h or longer to reduce the impact of
anticompaction caused by incremental repair.
| token_range_splitter.bytes_per_assignment | 50GiB | Configured to attempt repairing 50GiB of data per repair.
This throttles the amount of incremental repair and anticompaction done per schedule after incremental repairs are
turned on.
| token_range_splitter.max_bytes_per_schedule | 100GiB | The maximum number of bytes to cover in an individual schedule
to 100GiB. Consider increasing max_bytes_per_schedule if more data is written than this limit within
the min_repair_interval.
|===


=== Nodetool Configuration
==== nodetool getautorepairconfig
```
$> nodetool getautorepairconfig
repair scheduler configuration:
	repair_check_interval: 5m
	repair_max_retries: 3
	repair_retry_backoff: 30s
	repair_task_min_duration: 5s
	history_clear_delete_hosts_buffer_interval: 2h
configuration for repair_type: full
	enabled: true
	min_repair_interval: 24h
	repair_by_keyspace: false
	number_of_repair_threads: 1
	sstable_upper_threshold: 10000
	table_max_repair_time: 6h
	ignore_dcs: []
	repair_primary_token_range_only: true
	parallel_repair_count: 3
	parallel_repair_percentage: 3
	materialized_view_repair_enabled: false
	initial_scheduler_delay: 5m
	repair_session_timeout: 3h
	force_repair_new_node: false
	token_range_splitter: org.apache.cassandra.repair.autorepair.RepairTokenRangeSplitter
	token_range_splitter.bytes_per_assignment: 200GiB
	token_range_splitter.partitions_per_assignment: 1048576
	token_range_splitter.max_tables_per_assignment: 64
	token_range_splitter.max_bytes_per_schedule: 100000GiB
configuration for repair_type: incremental
	enabled: true
	min_repair_interval: 1h
	repair_by_keyspace: false
	number_of_repair_threads: 1
	sstable_upper_threshold: 10000
	table_max_repair_time: 6h
	ignore_dcs: []
	repair_primary_token_range_only: true
	parallel_repair_count: 3
	parallel_repair_percentage: 3
	materialized_view_repair_enabled: false
	initial_scheduler_delay: 5m
	repair_session_timeout: 3h
	force_repair_new_node: false
	token_range_splitter: org.apache.cassandra.repair.autorepair.RepairTokenRangeSplitter
	token_range_splitter.bytes_per_assignment: 50GiB
	token_range_splitter.partitions_per_assignment: 1048576
	token_range_splitter.max_tables_per_assignment: 64
	token_range_splitter.max_bytes_per_schedule: 100GiB
configuration for repair_type: preview_repaired
	enabled: false
```

==== nodetool autorepairstatus
```
$> nodetool autorepairstatus -t incremental
Active Repairs
425cea55-09aa-46e0-8911-9f37a4424574


$> nodetool autorepairstatus -t full
Active Repairs
NONE

```

==== nodetool setautorepairconfig
```
$> nodetool setautorepairconfig -t incremental number_of_repair_threads 2
```



==== More details
https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-37+Apache+Cassandra+Unified+Repair+Solution[CEP-37]